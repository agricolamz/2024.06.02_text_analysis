---
title: Инструменты для анализа текста в R
subtitle: летняя школа "Душа и процессор"
author: 
  - name: О. В. Алиева
  - name: Г. А. Мороз
date: today
date-format: D.MM.YYYY
format: html
execute:
  warning: false
  fig-width: 9
editor: source
lang: ru
df-print: paged
number-sections: true
bibliography: references.bib
toc: true
abstract: >
  Язык программирования R дает исследователю полный набор инструментов для 
  анализа текста. На занятии мы познакомимся с некоторыми из них, такими как 
  анализ частотности, векторизация и кластеризация. Кроме того, обсудим, какие 
  научные задачи могут решаться при помощи этих методов в историко-философских 
  и историко-культурных исследованиях. Опыт программирования не требуется.
editor_options: 
  chunk_output_type: console
---

```{r}
#| include: false

library(tidyverse)
theme_set(theme_bw())
```

## Введение в R

### Установка R и RStudio

Мы будем использовать R [@r_core_team], так что для занятий понадобятся:

- R
    - [на Windows](https://cran.r-project.org/bin/windows/base/)
    - [на Mac](https://cran.r-project.org/bin/macosx/)
    - [на Linux](https://cran.rstudio.com/bin/linux/), также можно установить из командной строки:
    
```
sudo apt-get install r-cran-base
```

- RStudio --- IDE для R ([можно скачать здесь](https://www.rstudio.com/products/rstudio/download/))

Часто можно увидеть или услышать, что R --- язык программирования для "статистической обработки данных". Изначально это, конечно, было правдой, но уже давно R --- это полноценный язык программирования, который при помощи своих пакетов позволяет решать огромный спектр задач. Мы будем использовать следующую версию R:

```{r}
#| echo: false

sessionInfo()$R.version$version.string |> cat()
```

Некоторые люди не любят устанавливать лишние программы себе на компьютер, несколько вариантов есть и для них:

- [RStudio cloud](https://posit.co/download/rstudio-desktop/) --- полная функциональность RStudio с некоторыми ограничениями;
- [webR REPL](https://webr.r-wasm.org/latest/) --- ограниченная версия компилятора R, которая работает в вашем браузере и не требует никаких установок на компьютер
- [Jupyter](https://jupyter.org/) ноутбуки;
- [Google Colab](https://colab.research.google.com) (нужно в настройках переключить ядро);
- [VS Code](https://code.visualstudio.com/) --- другое IDE, которое также позволяет работать с R;
- в принципе, в IDE нет нужды, можно работать из терминала, после установки, нужно всего лишь набрать `R`.

### Знакомство с RStudio

`RStudio` --- основной IDE для R. После установки R и RStudio можно открыть RStudio и перед вами предстанет что-то похожее на изображение ниже:

![RStudio при первом открытии](images/01_rstudio_initial_view.png)
После нажатия на двойное окошко чуть левее надписи *Environment* откроется окно скрипта.

![Подокна RStudio](images/02_rstudio_initial_view.png)

Все следующие команды можно 

- вводить в окне консоли, и тогда для исполнения следует нажимать клавишу `Enter`.
- вводить в окне скрипта, и тогда для исполнения следует нажимать клавиши `Ctrl/Cmd + Enter` или на команду Run на панели окна скрипта. Все, что введено в окне скрипта можно редактировать как в любом текстовом редакторе, в том числе сохранять `Ctrl/Cmd + S`.

### R как калькулятор

Давайте начнем с самого простого и попробуем использовать R как простой калькулятор. `+`, `-`, `*`, `/`, `^` (степень), `()` и т. д.

```{r}
40+2
3-2
5*6
99/9
2+4*2
(2+4)*2
2^3
```

### Создание переменных

```{r}
x <- "гнев, богиня, воспой"

y <- sqrt(2)
```

### Пайпы (конвееры)

В нашем коде мы часто будем использовать знаки конвеера (или пайпы): `|>`. Они призваны показывать последовательность действий.

```{r}
mean(sqrt(abs(sin(1:100))))

1:100 |> 
  sin() |> 
  abs() |> 
  sqrt() |> 
  mean()
```

Чаще всего можно останавливать посередине и смотреть, что на каком этапе трансформации данных получилось.

### Работа с пакетами

Все богатство R находиться в его огромной инфраструктуре пакетов, которые может разрабатывать кто угодно. Для сегодняшнего занятия нам понадобиться следующие пакеты: `tidyverse`, `tidytext`, `stopwords`, `broom`, `word2vec`, `uwot`. Чтобы их установить нужно использовать команду 

```{r}
#| eval: false

install.packages(c("tidyverse", "tidytext", "stopwords", "broom", "word2vec", "uwot"))
```

Помните, что если вы установили пакет, это не значит, что функции пакета вам доступны. Пакет еще нужно включить

![](images/03_install_packages.png)

Проверим, что все установилось, запустим пакет.

```{r}
library(tidyverse)
```

### Чтение текстовых файлов

В пакете `readr` (входит в `tidyverse`) для чтения текста есть функция `read_lines()`. В качестве первой переменной может выступать путь к файлу на компьютере или интернет ссылка:

```{r}
ion <- read_lines("https://raw.githubusercontent.com/agricolamz/2024.06.02_text_analysis/main/data/plato_w/ion.txt")
head(ion, 20)
```

В большинстве случаев, тексты получится считать, однако иногда при работе со старыми архивами могут возникнуть проблемы с кодировками, например, все тексты в старейшей интернет-библиотеке на русском языке --- библиотеке Максима Машкова ([lib.ru](lib.ru)) --- записаны в `KOI8-R`. В функциях пакета `readr` есть аргумент `locale`, который позволяет эксплицитно указать кодировку, а при считывании происходит процесс конвертации в стандартный для многих операционных систем `UTF-8`. Для текстов на русском языке важны следующие кодировки

- `KOI8-R`, а для украинского языка --- `KOI8-U`;
- `CP1251` (также известная под названием `Windows-1251`) покрывает и другие кириллические письменности такие как украинский, белорусский, болгарский, сербский, македонский и другие.

## Работа с корпусом текстов

### О корпусе

Для этого мастер-класса мы забрали русские переводы текстов Платоновского корпуса с сайта [Платоновского философского общества](https://plato.spbu.ru/TEXTS/index7.htm). Для скрапинга использовался пакет `rvest`. Для тех, кому интересно, -- код лежит по [ссылке](https://github.com/agricolamz/2024.06.02_text_analysis/blob/main/helping_scripts/scraping.R). 

Тексты на сайте приводятся по изданию: Платон. Сочинения в 4 тт. Под общей редакцией А. Ф. Лосева, В. Ф. Асмуса и А. А. Тахо-Годи. Серия «Философское наследие». 1990-1994.

Прежде чем сложить их в папку, мы удалили все латинские и греческие символы, цифры, а также все примечания. Код для этой предварительной "уборки" [здесь](https://github.com/agricolamz/2024.06.02_text_analysis/blob/main/helping_scripts/cleanTexts.R).

В итоге у нас получился корпус из 51 текста: это 31 текст из числа 36, входящих в тетралогии (без Писем, "Соперников", "Миноса", "Гиппарха" и "Клитофонта"),  при этом "Государство" и "Законы" разбиты по книгам (10 и 12, соответственно). Диалогов, не вошедших в тетралогии (так называемых spuria из Appendix Platonica), здесь тоже нет. 

:::{.callout-note}
В большинстве случаев анализировать автора по переводам -- не самое удачное решение (если только вы не изучаете сами переводы). Но мы хотели, чтобы происходящее на экране было понятно всем, а не только [антиварварам](https://t.me/antibarbari).
:::

### `tidy`-формат

Основные принципы опрятных данных:

- отдельный столбец для каждой переменной;
- отдельный ряд для каждого наблюдения;
- у каждого значения отдельная ячейка;
- один датасет – одна таблица.

![](https://d33wubrfki0l68.cloudfront.net/6f1ddb544fc5c69a2478e444ab8112fb0eea23f8/91adc/images/tidy-1.png)

C такими данными удобно работать, используя “грамматику трансформаци данных”, лежащую в основе диалекта `tidyverse`.  Эта грамматика предоставляет последовательный набор глаголов, которые помогают решать наиболее распространенные задачи манипулирования данными:

- `mutate()` добавляет новые или меняет старые переменные, которые являются функциями существующих переменных;
- `select()` выбирает переменные на основе их имен;
- `filter()` выбирает наблюдения на основе их значений;
- `summarise()` обобщает значения;
- `arrange()` изменяет порядок следования строк.

Все эти глаголы естественным образом сочетаются с функцией `group_by()`, которая позволяет выполнять любые операции “по группам”, и с оператором pipe `|>`.

### Загружаем наш корпус

Чтобы прочитать корпус текстов, укажите путь к ним из рабочей директории. Узнать, какая директория у вас рабочая, можно так:

```{r}
#| eval: false

getwd()
```

Изменить рабочую директорию можно из панели инструментов (вкладка `Session`, `Set Working Directory`) или при помощи функции `setwd()`, указав в качестве аргумента путь к рабочей директории на вашем компьютере (в кавычках, так как это символьный вектор). 

Загрузите в вашу рабочую директорию папку с корпусом текстов Платона. Ее можно найти по [ссылке](https://raw.githubusercontent.com/agricolamz/2024.06.02_text_analysis/main/data.zip).  


Папка `data` содержит две вложенные: `plato_w` c переводами Платона и `plato_l` с _лемматизированными_ текстами. Нам понадобятся обе, поместите `data` со всем содержимым в рабочую директорию. 

```
data
├── plato_l
│   ├── alkiviad_ii.txt
│   ├── ...
│   └── zakony_x.txt
└── plato_w
    ├── alkiviad_ii.txt
    ├── ...
    └── zakony_x.txt
```


:::{.callout-note}
Лемматизация --- автоматическое приведение слов к начальной форме (лемме). Здесь мы не будем рассказывать, как это делалось, но для интересующихся добавили [скрипт](https://github.com/agricolamz/2024.06.02_text_analysis/blob/main/helping_scripts/lemmatise.R) в репозиторий проекта. Можете попробовать запустить код самостоятельно. Для лемматизации использовалась модель [SynTagRus](https://universaldependencies.org/treebanks/ru_syntagrus/index.html). 
:::

```{r}
library(tidytext)
corpus <- list.files("data/plato_w", full.names = TRUE) |> 
  map_chr(read_lines) |> 
  tibble(doc_id = list.files("data/plato_w"),
         text = _,) |> 
  mutate(text = tolower(text),
         doc_id = str_remove(doc_id, "\\.txt"),
         doc_id = str_replace_all(doc_id, "_", " ")) 
```

Прочитаем файл .csv со сведениями о переводчиках диалогов. 

```{r}
#| message: false
translators <- read_csv("https://raw.githubusercontent.com/agricolamz/2024.06.02_text_analysis/main/translators.csv") 

translators

corpus <- corpus |> 
  left_join(translators)

corpus
```

## Анализ частотности

### Униграммы

```{r}
corpus |> 
  unnest_tokens(input = "text", output = "word", token = "words") |> 
  filter(str_detect(doc_id, "(timey)|(teetet)|(gorgiy)")) |> 
  count(doc_id, word) |> 
  group_by(doc_id) |> 
  slice_max(n = 10, order_by = n) |> 
  mutate(word = reorder_within(word, n, within = doc_id)) |> 
  ggplot(aes(n, word, fill = doc_id))+
  geom_col()+
  facet_wrap(~doc_id, scales = "free")+
  scale_y_reordered()+
  labs(x = NULL, y = NULL, fill = NULL)
```


Ожидаемо среди самых частотных оказались служебные части речи. В некоторых случаях (например, для кластеризации), это полезно, потому что они относительно независимы от тематики. В других от них лучше избавиться. Это можно сделать двумя способами: либо механически удалить стоп-слова, либо отобрать лексику на основе tf_idf.

### Стопслова и пакет `stopwords`

Стопслова для других язков можно раздобыть списки для других языков, используя пакет `stopwords`. Вместо имени языка, функция принимает ISO код языыка:

```{r}
library(stopwords)
stopwords("ru")
```

Пакет предоставляет несколько источников списков:

```{r}
stopwords_getsources()
```

Давайте посмотрим какие языки сейчас доступны:

```{r}
map(stopwords_getsources(), stopwords_getlanguages)
```

Мы видим, что есть несколько источников для русского языка:

```{r}
length(stopwords("ru", source = "snowball"))
length(stopwords("ru", source = "stopwords-iso"))
length(stopwords("ru", source = "marimo"))
length(stopwords("ru", source = "nltk"))
```

Создадим переменную, с которой мы будем работать дальше:

```{r}
stopwords_ru <- c(stopwords("ru", source = "stopwords-iso"), "таким", "образом", "коль", "скоро", "крайней", "мере", "совершенно", "верно", "итак", "давай", "вещи", "случае", "случаях", "словом", "либо", "которое", "ибо", "напротив", "стало", "твоему", "поскольку")

```

Список стоп-слов можно пополять, как показано выше, но главное не перестараться. Например, мы можем добавить туда формы существительного "часть", чтобы избавиться от оборотов вроде "по большей части" при подсчете биграм, но при этом есть риск потерять и важную информацию: проблематика части и целого важна для "Парменида". 

```{r}
idx <- which(stopwords_ru == "душа" | stopwords_ru == "жизнь")

stopwords_ru <- stopwords_ru[-idx]
```

### Удалим стоп-слова

Давайте теперь удалим стопслова и посмотрим на частотные слова в диалогах:

```{r}
corpus |> 
  unnest_tokens(input = "text", output = "word", token = "words") |>
  filter(str_detect(doc_id, "(timey)|(teetet)|(gorgiy)")) |> 
  anti_join(tibble(word = stopwords_ru)) |> 
  count(doc_id, word) |> 
  group_by(doc_id) |>
  slice_max(order_by = n, n = 10) |> 
  ungroup() |> 
  ggplot(aes(reorder_within(word, n, doc_id),
             n, fill = doc_id)) +
  geom_bar(stat="identity") +
  facet_wrap(~ doc_id, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(x = NULL, y = NULL, fill = NULL)
```


### Анализ биграмм

Аналогичным образом мы можем посчитать биграммы (то есть сочетания двух слов). 

```{r}
corpus |> 
  unnest_tokens(input = "text", output = "bigram", token = "ngrams", n = 2) |>
  filter(str_detect(doc_id, "(timey)|(teetet)|(gorgiy)")) |> 
  separate(col = bigram, into = c("item1", "item2"), sep = " ") |> 
  filter(!item1 %in% stopwords_ru, 
         !item2 %in% stopwords_ru) |>
  unite(bigram, c("item1", "item2"), sep = " ") |> 
  count(doc_id, bigram) |> 
  group_by(doc_id) |>
  slice_max(order_by = n, n = 10) |> 
  ungroup() |> 
  ggplot(aes(reorder_within(bigram, n, doc_id),
             n, fill = doc_id)) +
  geom_bar(stat="identity") +
  facet_wrap(~ doc_id, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(x = NULL, y = NULL, fill = NULL)
```

## Мера tf-idf


## Кластеризация на основе частотности языковых единиц

### Подготовка данных

Кластеризация относится к числу методов обучения без учителя для обнаружения неизвестных групп (кластеров) в данных. Точнее, это целый набор методов. Из них **иерархическая кластеризация** возвращает результат в виде дерева (дендрограммы), которая позволяет увидеть все возможные кластеры. Иерархическая кластеризация проводится обычно на основе самой частотной лексики.


```{r}
mfw <- corpus |> 
  unnest_tokens(input = "text", output = "word", token = "words") |>
  count(word) |> 
  slice_max(order_by = n, n = 100) |> 
  pull(word) 

mfw
```

Выберем переводчиков, у которых больше одного текста в корпусе.

```{r}
selected_translators <- translators |> 
  group_by(translator) |> 
  summarise(n = n()) |> 
  filter(n > 1) |> 
  pull(translator)

selected_translators
```

Теперь подготовим матрицу с частотностями. 

```{r}
corpus |> 
  filter(translator %in% selected_translators) |> 
  unnest_tokens(input = "text", output = "word", token = "words") |>
  count(doc_id, translator, word) |>
  group_by(doc_id) |> 
  mutate(total = sum(n),
         tf = n / total) |> 
  select(-n, -total) |> 
  filter(word %in% mfw) |> 
  pivot_wider(names_from = word, 
              values_from = tf, 
              values_fill = 0) 

# названия пригодятся при интерпретации
doc_id <- corpus_mx$doc_id
expected <- corpus_mx$translator

corpus_mx <- corpus_mx |> 
  select(-doc_id, -translator)
```

### Иерархическая кластеризация

Результаты кластеризации зависят от способа измерения объектов. Если одни объекты имеют больший разброс значений, чем другие, то при вычислении расстояний будут преобладать элементы с более широкими диапазонами.

Распространенное преобразование называется стандартизацией по Z-оценке: из значения признака Х вычитается среднее арифметическое, а результат делится на стандартное отклонение Х.

$$ X_{new} = \frac{X - Mean(X)}{StDev(X)}$$

Все средние после стандартизации должны быть в районе нуля. 

```{r}
corpus_mx <- corpus_mx |> 
  scale()
  
corpus_mx |>
  colMeans() |> 
  round(1) |>  
  head()
```

Вид дерева будет зависеть от того, какой тип присоединения вы выберете. Обычно предпочитают среднее и полное, т.к. они приводят к более сбалансированным дендрограммам. Для функции `hclust()` в R по умолчанию выставлено значение аргумента `method = "complete"`.

Применим алгоритм к данным о переводчиках Платона. Функция `dist()` по умолчанию считает евклидово расстояние. 

```{r}
rownames(corpus_mx) <- expected

corpus_mx |> 
  dist() |> 
  hclust(method = "complete") |> 
  plot()
```


Однако вычисления расстояния между текстами лучше подойдет не евклидово, а косинусное расстояние. В базовой `dist()` его нет, поэтому воспользуемся пакетом `philentropy`.

```{r}
dist_mx <- corpus_mx |> 
  philentropy::distance(method = "cosine", use.row.names = TRUE) 
```

Преобразуем меру сходства в меру расстояния и передадим на кластеризацию. 

```{r}
dist_mx <- as.dist(1 - dist_mx)
hc <- hclust(dist_mx)

plot(hc)
```

Для получения меток кластеров, возникающих в результате рассечения дендрограммы на той или иной высоте, можно воспользоваться функцией `cutree()`.

```{r}
cutree(hc, 5) |> head()
```

Этим меткам можно назначить свой цвет.

```{r}
#| fig-height: 10
#| message: false

library(dendextend)
hcd <- as.dendrogram(hc)
par(mar=c(2,2,2,7))
hcd %>% 
  set("branches_k_color", k = 6) %>% 
  set("labels_col", k=6) %>% 
  plot(horiz = TRUE)
abline(v=0.8, col="pink4",lty=2)
```

Важно помнить, что вид дерева может меняться в зависимости от числа mfw, выбранного вида связи и метрики расстояния. Чтобы определить наиболее устойчивые связи, применяют консенсусные деревья или консенсусные сети. 


## Векторизация

Word2vec -- это полносвязаная нейросеть с одним скрытым слоем. Такое обучение называется не глубоким, а поверхностным (shallow).

```{r word2vec}
#| cache: true

library(word2vec)

corpus <- tibble(doc_id = list.files("data/plato_l"),
                 text = map_chr(list.files("data/plato_l", full.names = TRUE), read_lines)) |> 
          mutate(text = tolower(text))

# устанавливаем зерно, т.к. начальные веса устанавливаются произвольно
set.seed(1234) 
model <- word2vec(x = corpus$text, 
                  type = "skip-gram",
                  dim = 50,
                  window = 5,
                  iter = 20,
                  hs = TRUE,
                  min_count = 5,
                  stopwords = stopwords_ru,
                  threads = 6)

```

Наша модель содержит эмбеддинги для слов; посмотрим на матрицу.

```{r}
emb <- as.matrix(model)
dim(emb)
```

Создатели алгоритма [утверждают](http://arxiv.org/pdf/1310.4546), что эмбеддинги можно осмысленно складывать и вычитать. Проверим. 

```{r}
vector <- emb["душа", ] + emb["мудрость", ]
predict(model, vector, type = "nearest", top_n = 10)
```

Кажется, это не совсем лишено смысла.  

```{r}
predict(model, c("дружба", "казнь", "учитель"), type = "nearest", top_n = 10) |> 
  bind_rows()
```


Получившуюся модель можно визуализировать. Для этого многомерное пространство нужно уменьшить до двух. Мы будем использовать алгоритм UMAP:

```{r umap}
#| cache: true

library(uwot)
viz <- umap(emb,  n_neighbors = 15, n_threads = 2)

dim(viz)
```


```{r}
#| fig-width: 15
#| fig-height: 15

tibble(word = rownames(emb), 
       V1 = viz[, 1], 
       V2 = viz[, 2]) |> 
  ggplot(aes(x = V1, y = V2, label = word)) + 
  geom_text(size = 2, alpha = 0.4)
```

```{r}
tibble(word = rownames(emb), 
       V1 = viz[, 1], 
       V2 = viz[, 2]) |>
  filter(V2 > 2.5) |> 
  ggplot(aes(x = V1, y = V2, label = word)) + 
  geom_text(size = 2, alpha = 0.4)
```


## Ссылки {.unnumbered}