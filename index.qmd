---
title: Инструменты для анализа текста в R
subtitle: летняя школа "Душа и процессор"
author: 
  - name: О. В. Алиева
  - name: Г. А. Мороз
date: today
date-format: D.MM.YYYY
format: html
execute:
  message: false
editor: source
lang: ru
df-print: paged
number-sections: true
bibliography: references.bib
toc: true
abstract: >
  Язык программирования R дает исследователю полный набор инструментов для 
  анализа текста. На занятии мы познакомимся с некоторыми из них, такими как 
  анализ частотности, векторизация и кластеризация. Кроме того, обсудим, какие 
  научные задачи могут решаться при помощи этих методов в историко-философских 
  и историко-культурных исследованиях. Опыт программирования не требуется.
editor_options: 
  chunk_output_type: console
---

```{r}
#| include: false

library(tidyverse)
theme_set(theme_minimal())
```

## Введение в R

### Установка R и RStudio

Мы будем использовать R [@r_core_team], так что для занятий понадобятся:

- R
    - [на Windows](https://cran.r-project.org/bin/windows/base/)
    - [на Mac](https://cran.r-project.org/bin/macosx/)
    - [на Linux](https://cran.rstudio.com/bin/linux/), также можно установить из командной строки:
    
```
sudo apt-get install r-cran-base
```

- RStudio --- IDE для R ([можно скачать здесь](https://www.rstudio.com/products/rstudio/download/))

Часто можно увидеть или услышать, что R --- язык программирования для "статистической обработки данных". Изначально это, конечно, было правдой, но уже давно R --- это полноценный язык программирования, который при помощи своих пакетов позволяет решать огромный спектр задач. Мы будем использовать следующую версию R:

```{r}
#| echo: false

sessionInfo()$R.version$version.string |> cat()
```

Некоторые люди не любят устанавливать лишние программы себе на компьютер, несколько вариантов есть и для них:

- [RStudio cloud](https://posit.co/download/rstudio-desktop/) --- полная функциональность RStudio с некоторыми ограничениями;
- [webR REPL](https://webr.r-wasm.org/latest/) --- ограниченная версия компилятора R, которая работает в вашем браузере и не требует никаких установок на компьютер
- [Jupyter](https://jupyter.org/) ноутбуки;
- [Google Colab](https://colab.research.google.com) (нужно в настройках переключить ядро);
- [VS Code](https://code.visualstudio.com/) --- другое IDE, которое также позволяет работать с R;
- в принципе, в IDE нет нужды, можно работать из терминала, после установки, нужно всего лишь набрать `R`.

### Знакомство с RStudio

`RStudio` --- основной IDE для R. После установки R и RStudio можно открыть RStudio и перед вами предстанет что-то похожее на изображение ниже:

![RStudio при первом открытии](images/01_rstudio_initial_view.png)
После нажатия на двойное окошко чуть левее надписи *Environment* откроется окно скрипта.

![Подокна RStudio](images/02_rstudio_initial_view.png)

Все следующие команды можно 

- вводить в окне консоли, и тогда для исполнения следует нажимать клавишу `Enter`.
- вводить в окне скрипта, и тогда для исполнения следует нажимать клавиши `Ctrl/Cmd + Enter` или на команду Run на панели окна скрипта. Все, что введено в окне скрипта можно редактировать как в любом текстовом редакторе, в том числе сохранять `Ctrl/Cmd + S`.

### R как калькулятор

Давайте начнем с самого простого и попробуем использовать R как простой калькулятор. `+`, `-`, `*`, `/`, `^` (степень), `()` и т. д.

```{r}
40+2
3-2
5*6
99/9
2+4*2
(2+4)*2
2^3
```

### Создание переменных

```{r}
x <- "гнев, богиня, воспой"

y <- sqrt(2)
```

### Пайпы (конвееры)

В нашем коде мы часто будем использовать знаки конвеера (или пайпы): `|>`. Они призваны показывать последовательность действий.

```{r}
mean(sqrt(abs(sin(1:100))))

1:100 |> # `|` + `>` получается |>
  sin() |> 
  abs() |> 
  sqrt() |> 
  mean()
```

Чаще всего можно останавливать посередине и смотреть, что на каком этапе трансформации данных получилось.

### Работа с пакетами

Все богатство R находиться в его огромной инфраструктуре пакетов, которые может разрабатывать кто угодно. Для сегодняшнего занятия нам понадобиться следующие пакеты: `tidyverse`, `tidytext`, `stopwords`, `word2vec`, `uwot`. Чтобы их установить нужно использовать команду 

```{r}
#| eval: false

install.packages(c("tidyverse", "tidytext", "stopwords", "word2vec", "uwot"))
```

Помните, что если вы установили пакет, это не значит, что функции пакета вам доступны. Пакет еще нужно включить

![](images/03_install_packages.png)

Проверим, что все установилось, запустим пакет.

```{r}
library(tidyverse)
```

### Чтение текстовых файлов

В пакете `readr` (входит в `tidyverse`) для чтения текста есть функция `read_lines()`. В качестве первой переменной может выступать путь к файлу на компьютере или интернет ссылка:

```{r}
ion <- read_lines("https://raw.githubusercontent.com/agricolamz/2024.06.02_text_analysis/main/data/plato_w/ion.txt")
head(ion, 20)
```

В большинстве случаев, тексты получится считать, однако иногда при работе со старыми архивами могут возникнуть проблемы с кодировками, например, все тексты в старейшей интернет-библиотеке на русском языке --- библиотеке Максима Машкова ([lib.ru](lib.ru)) --- записаны в `KOI8-R`. В функциях пакета `readr` есть аргумент `locale`, который позволяет эксплицитно указать кодировку, а при считывании происходит процесс конвертации в стандартный для многих операционных систем `UTF-8`. Для текстов на русском языке важны следующие кодировки

- `KOI8-R`, а для украинского языка --- `KOI8-U`;
- `CP1251` (также известная под названием `Windows-1251`) покрывает и другие кириллические письменности такие как украинский, белорусский, болгарский, сербский, македонский и другие.

## Работа с корпусом текстов

### `tidy`-формат

### Считываем наш корпус

Чтобы прочитать корпус текстов, укажите путь к ним из рабочей директории. Узнать, какая директория у вас рабочая, можно так:

```{r}
#| eval: false

getwd()
```

Изменить рабочую директорию можно из панели инструментов (вкладка `Session`, `Set Working Directory`) или при помощи функции `setwd()`, указав в качестве аргумента путь к рабочей директории на вашем компьютере (в кавычках, так как это символьный вектор). 

Загрузите в вашу рабочую директорию файлы с корпусом текстов Платона. Их можно найти по [ссылке](https://raw.githubusercontent.com/agricolamz/2024.06.02_text_analysis/main/data.zip).  


Папка `data` содержит две вложенные: `plato_w` c переводами Платона и `plato_l` с лемматизированными текстами. Нам понадобятся обе, поместите `data` со всем содержимым в рабочую директорию. 

Узнаем, какие файлы лежат в папке `plato_l`:

```{r}
filenames <- list.files("./data/plato_l", full.names = TRUE) 
head(filenames) 
length(filenames)
```

Объект `filenames` -- это символьный вектор, в котором 51 элемент. В нем хранятся пути до файлов с диалогами. Функция `head()` выводит только первые шесть элементов

Теперь наша задача -- прочитать их все в R, чтобы можно было посчитать частотности. Для этого нужен цикл, который продется по всем файлам и соберет оттуда текст. 

```{r}
corpus <- map(filenames, read_lines)
```

Теперь у вас в окружении появился список `corpus`, в котором 51 элемент, по числу файлов. Достать любой элемент из списка и посмотреть на него можно по индексу. Текст большой, поэтому снова выведем только начало.

```{r}
head(corpus[[1]], 29)
```

Каждому элементу списка можно присвоить имя:

```{r}
cnames <- list.files("./data/plato_l", full.names = FALSE) |> 
  str_remove("\\.txt")

names(corpus) <- cnames
```

Нажмите на объект в окружении, чтобы понять, что изменилось.

## Анализ частотности

### Униграммы

В нашем корпусе текст уже разделен на отдельные слова (и знаки препинания), потому что предварительно мы его _лемматизировали_. Это значит, что все слова были автоматически приведены к начальной форме (лемме). 

Прежде чем их считать, удобно превратить объект `corpus` из списка в таблицу (tibble), чтобы можно было работать с использованием `tidyverse`.

```{r}
corpus_tbl <- corpus  |> 
  stack() |> 
  as_tibble() |>  
  transmute(text = ind, word = values)

head(corpus_tbl)
nrow(corpus_tbl)
```

У нас получилась очень длинная таблица, в которой хранятся все слова и знаки препинания для диалогов. Знаки препинания нам не интересны, поэтому сразу от них избавимся.

```{r}
corpus_tbl <- corpus_tbl |>  
  filter(!str_detect(word, "[[:punct:]<>]")) 
```

Узнаем, сколько слов в каждом тексте:

```{r}
total <- corpus_tbl |>  
  group_by(text) |> 
  summarise(total = n()) |> 
  arrange(-total)

total
```

Нам осталось посчитать частотность для каждого слова и разделить на общее число слов. Таким образом мы узнаем относительную частотность (tf) для каждого слова, и сможем отобрать наиболее частотные или, наоборот, редкие слова. Обратите внимание, что таблица `corpus_counts` уже не такая длинная, как `corpus_tbl`.

```{r}
corpus_counts <- corpus_tbl |>  
  count(text, word) |> 
  arrange(-n)

head(corpus_counts)
nrow(corpus_counts)
```

Ожидаемо среди самых частотных единиц оказались служебные части речи. 

Соединим две таблицы.

```{r}
corpus_tf <- corpus_counts |> 
  left_join(total)

corpus_tf
```

Теперь добавим новый столбец:

```{r}
corpus_tf <- corpus_tf |> 
  mutate(tf = n / total) |> 
  arrange(-tf)

corpus_tf
```

Для большинства диалогов список наиболее частотных слов будет выглядеть так:

```{r}
library(tidytext)
corpus_tf |> 
  filter(text %in% c("timey", "teetet", "gorgiy")) |> 
  group_by(text) |> 
  slice_max(order_by = tf, n = 10) |> 
  ungroup() |> 
  ggplot(aes(reorder_within(word, tf, text),
             tf, fill = text)) +
  geom_bar(stat="identity", show.legend = FALSE) +
  facet_wrap(~ text, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  xlab(NULL)
```

В некоторых случаях (например, для кластеризации), такие служебные слова нужны, потому что они относительно независимы от тематики. В других случаях (например, для тематического моделирования) от них лучше избавиться. Это можно сделать двумя способами: либо механически удалить стоп-слова, либо отобрать лексику на основе tf_idf.

### Стопслова и пакет `stopwords`

Стопслова для других язков можно раздобыть списки для других языков, используя пакет `stopwords`. Вместо имени языка, функция принимает ISO код языыка:

```{r}
library(stopwords)
stopwords("ru")
```

Пакет предоставляет несколько источников списков:

```{r}
stopwords_getsources()
```

Давайте посмотрим какие языки сейчас доступны:

```{r}
map(stopwords_getsources(), stopwords_getlanguages)
```

Мы видим, что есть несколько источников для русского языка:

```{r}
length(stopwords("ru", source = "snowball"))
length(stopwords("ru", source = "stopwords-iso"))
length(stopwords("ru", source = "marimo"))
length(stopwords("ru", source = "nltk"))
```

Создадим переменную, с которой мы будем работать дальше:

```{r}
stopwords_ru <- stopwords("ru", source = "stopwords-iso")
```


### Удалим стоп слова

Давайте теперь удалим стопслова и посмотрим на частотные слова в диалогах:

```{r}
corpus_tf |> 
  filter(text %in% c("timey", "teetet", "gorgiy")) |> 
  anti_join(tibble(word = stopwords_ru)) |> 
  group_by(text) |> 
  slice_max(order_by = tf, n = 10) |> 
  ungroup() |> 
  ggplot(aes(reorder_within(word, tf, text),
             tf, fill = text)) +
  geom_bar(stat="identity", show.legend = FALSE) +
  facet_wrap(~ text, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  xlab(NULL)
```

### Анализа биграм

## Мера tf-idf

## Анализ коллокаций

```{r}
frequency_range <- 1000

corpus_tbl$word |>
  str_c(collapse = " ") |> 
  tibble(text = _) |> 
  unnest_tokens(output = "bigram", input = text, token = "ngrams", n = 2) |> 
  na.omit() |> 
  separate(bigram, into = c("word1", "word2"), sep = " ") |> 
  anti_join(tibble(word1 = stopwords_ru)) |> 
  anti_join(tibble(word2 = stopwords_ru)) |>
  count(word1, word2) |> 
  slice_max(n = frequency_range, n) ->
  bigram_count

# create unigram count  ---------------------------------------------------

corpus_tbl |> 
  count(word) |> 
  mutate(total = sum(n))->
  word_count

# merge them all and calculate measures -----------------------------------

bigram_count |> 
  rename(O11 = n) |> 
  left_join(word_count |> select(-total), by = c("word1" = "word")) |> 
  rename(O12 = n) |> 
  left_join(word_count, by = c("word2" = "word")) |> 
  rename(O21 = n) |> 
  mutate(O21 = as.double(O21),
         O12 = as.double(O12),
         O11 = as.double(O11),
         O21 = O21 - O11,
         O12 = O12 - O11,
         O22 = total - O12 - O21 - O11,
         R1 = O11 + O12, 
         R2 = O21 + O22,
         C1 = O11 + O21, 
         C2 = O12 + O22,
         N = O11 + O12 + O21 + O22,
         E11 = R1 * C1 / N, 
         E12 = R1 * C2 / N,
         E21 = R2 * C1 / N, 
         E22 = R2 * C2 / N,
         MI = log2(O11 / E11),
         t.score = (O11 - E11) / sqrt(O11),
         X2 = (O11-E11)^2/E11 + (O12-E12)^2/E12 + (O21-E21)^2/E21 + (O22-E22)^2/E22,
         DP = O11 / R1 - O21 / R2) |> 
  arrange(desc(t.score)) |> 
  select(word1, word2, O11, O21, O12, N, MI, t.score, X2, DP) |> 
  rename(total = N,
         co_occurrence_frequency = O11,
         w1_frequency = O12,
         w2_frequency = O21)
```


## Кластеризация авторов на основе частотности языковых единиц

## Векторизация

```{r}
set.seed(20240602)
library(word2vec)
model <- word2vec(x = corpus_tbl$word)
embedding <- as.matrix(model)
dim(embedding)
predict(model, c("дружба", "казнь", "учитель"), type = "nearest", top_n = 10) |> 
  bind_rows()
```

Получившуюся модель можно визуализировать. Для этого многомерное пространство нужно уменьшить до двух. Мы будем использовать алгоритм UMAP:

```{r umap}
#| cache: true

library(uwot)
viz <- umap(embedding, n_neighbors = 15, n_threads = 2)
dim(viz)
```


```{r}
#| fig-width: 15
#| fig-height: 15

tibble(word = rownames(embedding), 
       x = viz[, 1], 
       y = viz[, 2]) |> 
  ggplot(aes(x = x, y = y, label = word)) + 
  geom_text(size = 2)
```

## Ссылки {.unnumbered}