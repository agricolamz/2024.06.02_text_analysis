---
title: Инструменты для анализа текста в R
subtitle: летняя школа "Душа и процессор"
author: 
  - name: О. В. Алиева
  - name: Г. А. Мороз
date: today
date-format: D.MM.YYYY
format: html
execute:
  message: false
editor: source
lang: ru
df-print: paged
number-sections: true
bibliography: references.bib
toc: true
abstract: >
  Язык программирования R дает исследователю полный набор инструментов для 
  анализа текста. На занятии мы познакомимся с некоторыми из них, такими как 
  анализ частотности, векторизация и кластеризация. Кроме того, обсудим, какие 
  научные задачи могут решаться при помощи этих методов в историко-философских 
  и историко-культурных исследованиях. Опыт программирования не требуется.
editor_options: 
  chunk_output_type: console
---

```{r}
#| include: false

library(tidyverse)
theme_set(theme_minimal())
```

## Введение в R

### Установка R и RStudio

Мы будем использовать R [@r_core_team], так что для занятий понадобятся:

- R
    - [на Windows](https://cran.r-project.org/bin/windows/base/)
    - [на Mac](https://cran.r-project.org/bin/macosx/)
    - [на Linux](https://cran.rstudio.com/bin/linux/), также можно установить из командной строки:
    
```
sudo apt-get install r-cran-base
```

- RStudio --- IDE для R ([можно скачать здесь](https://www.rstudio.com/products/rstudio/download/))

Часто можно увидеть или услышать, что R --- язык программирования для "статистической обработки данных". Изначально это, конечно, было правдой, но уже давно R --- это полноценный язык программирования, который при помощи своих пакетов позволяет решать огромный спектр задач. Мы будем использовать следующую версию R:

```{r}
#| echo: false

sessionInfo()$R.version$version.string |> cat()
```

Некоторые люди не любят устанавливать лишние программы себе на компьютер, несколько вариантов есть и для них:

- [RStudio cloud](https://posit.co/download/rstudio-desktop/) --- полная функциональность RStudio с некоторыми ограничениями;
- [webR REPL](https://webr.r-wasm.org/latest/) --- ограниченная версия компилятора R, которая работает в вашем браузере и не требует никаких установок на компьютер
- [Jupyter](https://jupyter.org/) ноутбуки;
- [Google Colab](https://colab.research.google.com) (нужно в настройках переключить ядро);
- [VS Code](https://code.visualstudio.com/) --- другое IDE, которое также позволяет работать с R;
- в принципе, в IDE нет нужды, можно работать из терминала, после установки, нужно всего лишь набрать `R`.

### Знакомство с RStudio

`RStudio` --- основной IDE для R. После установки R и RStudio можно открыть RStudio и перед вами предстанет что-то похожее на изображение ниже:

![RStudio при первом открытии](images/01_rstudio_initial_view.png)
После нажатия на двойное окошко чуть левее надписи *Environment* откроется окно скрипта.

![Подокна RStudio](images/02_rstudio_initial_view.png)

Все следующие команды можно 

- вводить в окне консоли, и тогда для исполнения следует нажимать клавишу `Enter`.
- вводить в окне скрипта, и тогда для исполнения следует нажимать клавиши `Ctrl/Cmd + Enter` или на команду Run на панели окна скрипта. Все, что введено в окне скрипта можно редактировать как в любом текстовом редакторе, в том числе сохранять `Ctrl/Cmd + S`.

### R как калькулятор

Давайте начнем с самого простого и попробуем использовать R как простой калькулятор. `+`, `-`, `*`, `/`, `^` (степень), `()` и т. д.

```{r}
40+2
3-2
5*6
99/9
2+4*2
(2+4)*2
2^3
```

### Создание переменных

```{r}
x <- "гнев, богиня, воспой"

y <- sqrt(2)
```

### Пайпы (конвееры)

В нашем коде мы часто будем использовать знаки конвеера (или пайпы): `|>`. Они призваны показывать последовательность действий.

```{r}
mean(sqrt(abs(sin(1:100))))

1:100 |> # `|` + `>` получается |>
  sin() |> 
  abs() |> 
  sqrt() |> 
  mean()
```

Чаще всего можно останавливать посередине и смотреть, что на каком этапе трансформации данных получилось.

### Работа с пакетами

Все богатство R находиться в его огромной инфраструктуре пакетов, которые может разрабатывать кто угодно. Для сегодняшнего занятия нам понадобиться следующие пакеты: `tidyverse`, `tidytext`, `stopwords`, `word2vec`, `uwot`. Чтобы их установить нужно использовать команду 

```{r}
#| eval: false

install.packages(c("tidyverse", "tidytext", "stopwords", "word2vec", "uwot"))
```

Помните, что если вы установили пакет, это не значит, что функции пакета вам доступны. Пакет еще нужно включить

![](images/03_install_packages.png)

Проверим, что все установилось, запустим пакет.

```{r}
library(tidyverse)
```

### Чтение текстовых файлов

В пакете `readr` (входит в `tidyverse`) для чтения текста есть функция `read_lines()`. В качестве первой переменной может выступать путь к файлу на компьютере или интернет ссылка:

```{r}
ion <- read_lines("https://raw.githubusercontent.com/agricolamz/2024.06.02_text_analysis/main/data/plato_w/ion.txt")
head(ion, 20)
```

В большинстве случаев, тексты получится считать, однако иногда при работе со старыми архивами могут возникнуть проблемы с кодировками, например, все тексты в старейшей интернет-библиотеке на русском языке --- библиотеке Максима Машкова ([lib.ru](lib.ru)) --- записаны в `KOI8-R`. В функциях пакета `readr` есть аргумент `locale`, который позволяет эксплицитно указать кодировку, а при считывании происходит процесс конвертации в стандартный для многих операционных систем `UTF-8`. Для текстов на русском языке важны следующие кодировки

- `KOI8-R`, а для украинского языка --- `KOI8-U`;
- `CP1251` (также известная под названием `Windows-1251`) покрывает и другие кириллические письменности такие как украинский, белорусский, болгарский, сербский, македонский и другие.

## Работа с корпусом текстов

### О корпусе

Для этого мастер-класса мы забрали русские переводы текстов Платоновского корпуса с сайта [Платоновского философского общества](https://plato.spbu.ru/TEXTS/index7.htm). Для скрапинга использовался пакет `rvest`. Для тех, кому интересно, -- код лежит по [ссылке](https://github.com/agricolamz/2024.06.02_text_analysis/blob/main/helping_scripts/scraping.R). 

Тексты на сайте приводятся по изданию: Платон. Сочинения в 4 тт. Под общей редакцией А. Ф. Лосева, В. Ф. Асмуса и А. А. Тахо-Годи. Серия «Философское наследие». 1990-1994.

Прежде чем сложить их в папку, мы удалили все латинские и греческие символы, цифры, а также все примечания. Код для этой предварительной "уборки" [здесь](https://github.com/agricolamz/2024.06.02_text_analysis/blob/main/helping_scripts/cleanTexts.R).

В итоге у нас получился корпус из 51 текста: это 31 текст из числа 36, входящих в тетралогии (без Писем, "Соперников", "Миноса", "Гиппарха" и "Клитофонта"),  при этом "Государство" и "Законы" разбиты по книгам (10 и 12, соответственно). Диалогов, не вошедших в тетралогии (так называемых spuria из Appendix Platonica), здесь тоже нет. 

:::{.callout-note}
В большинстве случаев анализировать автора по переводам -- не самое удачное решение (если только вы не изучаете сами переводы). Но мы хотели, чтобы происходящее на экране было понятно всем, а не только [антиварварам](https://t.me/antibarbari).
:::

### `tidy`-формат

Основные принципы опрятных данных:

- отдельный столбец для каждой переменной;
- отдельный ряд для каждого наблюдения;
- у каждого значения отдельная ячейка;
- один датасет – одна таблица.

![](https://d33wubrfki0l68.cloudfront.net/6f1ddb544fc5c69a2478e444ab8112fb0eea23f8/91adc/images/tidy-1.png)

C такими данными удобно работать, используя “грамматику манипуляции данных”, лежащую в основе `dplyr` и других пакетов семейства `tidyverse`.  Эта грамматика предоставляет последовательный набор глаголов, которые помогают решать наиболее распространенные задачи манипулирования данными:

- `mutate()` добавляет новые переменные, которые являются функциями существующих переменных;
- `select()` выбирает переменные на основе их имен;
- `filter()` выбирает наблюдения на основе их значений;
- `summarise()` обобщает значения;
- `arrange()` изменяет порядок следования строк.

Все эти глаголы естественным образом сочетаются с функцией `group_by()`, которая позволяет выполнять любые операции “по группам”, и с оператором pipe `%>%` из пакета `magrittr`.

Дальше мы попробуем использовать некоторые из этих функций.

### Считываем наш корпус

Чтобы прочитать корпус текстов, укажите путь к ним из рабочей директории. Узнать, какая директория у вас рабочая, можно так:

```{r}
#| eval: false

getwd()
```

Изменить рабочую директорию можно из панели инструментов (вкладка `Session`, `Set Working Directory`) или при помощи функции `setwd()`, указав в качестве аргумента путь к рабочей директории на вашем компьютере (в кавычках, так как это символьный вектор). 

Загрузите в вашу рабочую директорию папку с корпусом текстов Платона. Ее можно найти по [ссылке](https://raw.githubusercontent.com/agricolamz/2024.06.02_text_analysis/main/data.zip).  


Папка `data` содержит две вложенные: `plato_w` c переводами Платона и `plato_l` с _лемматизированными_ текстами. Нам понадобятся обе, поместите `data` со всем содержимым в рабочую директорию. 


:::{.callout-note}
Лемматизация -- автоматическое приведение слов к начальной форме (лемме). Здесь мы не будем рассказывать, как это делалось, но для интересующихся добавили [скрипт](https://github.com/agricolamz/2024.06.02_text_analysis/blob/main/helping_scripts/lemmatise.R) в репозиторий проекта. Можете попробовать запустить код самостоятельно. Для лемматизации использовалась модель [SynTagRus](https://universaldependencies.org/treebanks/ru_syntagrus/index.html). 
:::


Для начала узнаем, какие файлы лежат в папке `plato_w`:

```{r}
filenames <- list.files("./data/plato_w", full.names = TRUE) 
head(filenames) 
length(filenames)
```

Объект `filenames` -- это символьный вектор, в котором 51 элемент. В нем хранятся пути до файлов с диалогами. Функция `head()` выводит только первые шесть элементов

Теперь наша задача -- прочитать их все в R, чтобы можно было посчитать частотности. Для этого нужен цикл, который продется по всем файлам и соберет оттуда текст. 

```{r}
corpus <- map(filenames, read_lines)
```

Теперь у вас в окружении появился список `corpus`, в котором 51 элемент, по числу файлов. Достать любой элемент из списка и посмотреть на него можно по индексу. Текст большой, поэтому снова выведем только начало.

```{r}
corpus[[1]]
```

Каждому элементу списка можно присвоить имя:

```{r}
cnames <- list.files("./data/plato_w", full.names = FALSE) |> 
  str_remove("\\.txt")

names(corpus) <- cnames
```

Нажмите на объект в окружении, чтобы понять, что изменилось.

## Анализ частотности

### Униграммы

Прежде всего превратим объект `corpus` из списка в таблицу (tibble), чтобы можно было работать с использованием `tidyverse`.

```{r}
corpus_tbl <- corpus  |> 
  stack() |> 
  as_tibble() |>  
  transmute(id = ind, text = values)

head(corpus_tbl)
nrow(corpus_tbl)
```

У нас получилась таблица, в которой хранятся тексты диалогов. Разобьем каждый текст на слова. Теперь наши данные хранятся в опрятном виде.

```{r}
library(tidytext)
corpus_w <- corpus_tbl |> 
  unnest_tokens(word, text) 

corpus_w
```

Узнаем, сколько слов в каждом тексте:

```{r}
total <- corpus_w |>  
  group_by(id) |> 
  summarise(total = n()) |> 
  arrange(-total)

total
```

Нам осталось посчитать частотность для каждого слова и разделить на общее число слов. Таким образом мы узнаем относительную частотность (tf) для каждого слова, и сможем отобрать наиболее частотные или, наоборот, редкие слова. Обратите внимание, что таблица `corpus_counts` уже не такая длинная, как `corpus_tbl`.

```{r}
corpus_counts <- corpus_w |>  
  count(id, word) |> 
  arrange(-n)

head(corpus_counts)
nrow(corpus_counts)
```

Ожидаемо среди самых частотных единиц оказались служебные части речи. 

Соединим две таблицы.

```{r}
corpus_tf <- corpus_counts |> 
  left_join(total)

corpus_tf
```

Теперь добавим новый столбец:

```{r}
corpus_tf <- corpus_tf |> 
  mutate(tf = n / total) |> 
  arrange(-tf)

corpus_tf
```

Для большинства диалогов список наиболее частотных слов будет выглядеть так:

```{r}
corpus_tf |> 
  filter(id %in% c("timey", "teetet", "gorgiy")) |>
  group_by(id) |> 
  slice_max(order_by = tf, n = 10) |> 
  ungroup() |> 
  ggplot(aes(reorder_within(word, tf, id),
             tf, fill = id)) +
  geom_bar(stat="identity", show.legend = FALSE) +
  facet_wrap(~ id, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  xlab(NULL)
```

В некоторых случаях (например, для кластеризации), такие служебные слова нужны, потому что они относительно независимы от тематики. В других случаях (например, для тематического моделирования) от них лучше избавиться. Это можно сделать двумя способами: либо механически удалить стоп-слова, либо отобрать лексику на основе tf_idf.

### Стопслова и пакет `stopwords`

Стопслова для других язков можно раздобыть списки для других языков, используя пакет `stopwords`. Вместо имени языка, функция принимает ISO код языыка:

```{r}
library(stopwords)
stopwords("ru")
```

Пакет предоставляет несколько источников списков:

```{r}
stopwords_getsources()
```

Давайте посмотрим какие языки сейчас доступны:

```{r}
map(stopwords_getsources(), stopwords_getlanguages)
```

Мы видим, что есть несколько источников для русского языка:

```{r}
length(stopwords("ru", source = "snowball"))
length(stopwords("ru", source = "stopwords-iso"))
length(stopwords("ru", source = "marimo"))
length(stopwords("ru", source = "nltk"))
```

Создадим переменную, с которой мы будем работать дальше:

```{r}
stopwords_ru <- c(stopwords("ru", source = "stopwords-iso"), "таким", "образом", "коль", "скоро", "крайней", "мере", "совершенно", "верно", "итак", "давай", "вещи", "случае", "случаях", "словом", "либо", "которое", "ибо", "напротив", "стало", "твоему", "поскольку")

```

Список стоп-слов можно пополять, как показано выше, но главное не перестараться. Например, мы можем добавить туда формы существительного "часть", чтобы избавиться от оборотов вроде "по большей части" при подсчете биграм, но при этом есть риск потерять и важную информацию: проблематика части и целого важна для "Парменида". 

### Удалим стоп-слова

Давайте теперь удалим стопслова и посмотрим на частотные слова в диалогах:

```{r}
corpus_tf |> 
  filter(id %in% c("timey", "teetet", "gorgiy")) |> 
  anti_join(tibble(word = stopwords_ru)) |> 
  group_by(id) |> 
  slice_max(order_by = tf, n = 10) |> 
  ungroup() |> 
  ggplot(aes(reorder_within(word, tf, id),
             tf, fill = id)) +
  geom_bar(stat="identity", show.legend = FALSE) +
  facet_wrap(~ id, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  xlab(NULL)
```

О чем вам говорят эти диаграммы?

### Анализ биграмм

Аналогичным образом мы можем посчитать биграммы (то есть сочетания двух слов). 

```{r}
corpus_b <- corpus_tbl |> 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) 

corpus_b
```

Теперь удалим стоп-слова и посчитаем биграммы.

```{r}
count_bigrams <- corpus_b  |> 
  separate(col = bigram, into = c("item1", "item2"), sep = " ") |> 
  filter(!item1 %in% stopwords_ru, 
         !item2 %in% stopwords_ru) |>
  count(id, item1, item2, sort = T) |> 
  unite(bigram, c("item1", "item2"), sep = " ")

count_bigrams
```

Вот так выглядят наиболее частотные биграммы для тех же трех диалогов:

```{r}
count_bigrams |> 
  filter(id %in% c("timey", "teetet", "gorgiy")) |> 
  group_by(id) |> 
  slice_max(order_by = n, n = 10) |> 
  ungroup() |> 
  ggplot(aes(reorder_within(bigram, n, id),
             n, fill = id)) +
  geom_bar(stat="identity", show.legend = FALSE) +
  facet_wrap(~ id, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  xlab(NULL)

```

## Мера tf-idf

## Анализ коллокаций

```{r}
frequency_range <- 1000

corpus_w$word |>
  str_c(collapse = " ") |> 
  tibble(text = _) |> 
  unnest_tokens(output = "bigram", input = text, token = "ngrams", n = 2) |> 
  na.omit() |> 
  separate(bigram, into = c("word1", "word2"), sep = " ") |> 
  anti_join(tibble(word1 = stopwords_ru)) |> 
  anti_join(tibble(word2 = stopwords_ru)) |>
  count(word1, word2) |> 
  slice_max(n = frequency_range, n) ->
  bigram_count

# create unigram count  ---------------------------------------------------

corpus_w |> 
  count(word) |> 
  mutate(total = sum(n))->
  word_count

# merge them all and calculate measures -----------------------------------

bigram_count |> 
  rename(O11 = n) |> 
  left_join(word_count |> select(-total), by = c("word1" = "word")) |> 
  rename(O12 = n) |> 
  left_join(word_count, by = c("word2" = "word")) |> 
  rename(O21 = n) |> 
  mutate(O21 = as.double(O21),
         O12 = as.double(O12),
         O11 = as.double(O11),
         O21 = O21 - O11,
         O12 = O12 - O11,
         O22 = total - O12 - O21 - O11,
         R1 = O11 + O12, 
         R2 = O21 + O22,
         C1 = O11 + O21, 
         C2 = O12 + O22,
         N = O11 + O12 + O21 + O22,
         E11 = R1 * C1 / N, 
         E12 = R1 * C2 / N,
         E21 = R2 * C1 / N, 
         E22 = R2 * C2 / N,
         MI = log2(O11 / E11),
         t.score = (O11 - E11) / sqrt(O11),
         X2 = (O11-E11)^2/E11 + (O12-E12)^2/E12 + (O21-E21)^2/E21 + (O22-E22)^2/E22,
         DP = O11 / R1 - O21 / R2) |> 
  arrange(desc(t.score)) |> 
  select(word1, word2, O11, O21, O12, N, MI, t.score, X2, DP) |> 
  rename(total = N,
         co_occurrence_frequency = O11,
         w1_frequency = O12,
         w2_frequency = O21)
```


## Кластеризация на основе частотности языковых единиц

### Способы кластеризации
Все методы машинного обучения делятся на методы **обучения с учителем** и методы обучения без учителя. В первом случае у нас есть некоторое количество признаков X, измеренных у n объектов, и некоторый отклик Y. Задача заключается в предсказании Y по X. Например, мы измерили вес и пушистость у сотни котов известных пород, и хотим предсказать породу других котов, зная их вес и пушистость.

**Обучение без учителя** предназначено для случаев, когда у нас есть только некоторый набор признаков X, но нет значения отклика. Например, есть группа котов, для которых мы измерили вес и пушистость, но мы не знаем, на какие породы они делятся. 

**Кластеризация** относится к числу методов для обнаружения неизвестных групп (кластеров) в данных. Точнее, это целый набор методов. Мы рассмотрим два из них: 
- кластеризация по методу K средних
- иерархическая кластеризация

В случае с кластеризацией по методу K средних мы пытаемся разбить наблюдения на некоторое заранее заданное число кластеров. Иерархическая кластеризация возвращает результат в виде дерева (**дендрограммы**), которая позволяет увидеть все возможные кластеры.

### Подготовка данных

Сначала прочитаем файл .csv со сведениями о переводчиках диалогов. 

```{r}
url <- "https://raw.githubusercontent.com/agricolamz/2024.06.02_text_analysis/main/data/translators.csv"
translators <- read_delim(url, delim = ";") |> 
  mutate(translator = str_remove(translator, "\\w\\.\\w\\.")) |> 
  mutate(translator = str_remove_all(translator, " "))

translators
```

Теперь соединим эту таблицу с объектом `corpus_tf`, где хранится относительная частотность для токенов (до удаления стоп-слов). 

```{r}
corpus_trans <- corpus_tf |> 
  select(-n, -total) |> 
  left_join(translators)

corpus_trans
```

Выберем 300 наиболее частотных слов. 

```{r}
mfw <- corpus_counts |> 
  group_by(word) |> 
  summarise(counts = sum(n)) |> 
  arrange(-counts) |> 
  slice_head(n = 300) |> 
  pull(word)

mfw
```

Удалим переводчиков, у которых только один текст в корпусе.

```{r}
selected_translators <- translators |> 
  group_by(translator) |> 
  summarise(n = n()) |> 
  filter(n > 1) |> 
  pull(translator)

selected_translators
```

Теперь подготовим матрицу с частотностями. 

```{r}
corpus_mx <- corpus_trans |> 
  filter(word %in% mfw) |> 
  filter(translator %in% selected_translators) |> 
  pivot_wider(names_from = word, 
              values_from = tf, 
              values_fill = 0) |> 
  select(-id)

corpus_mx
```

Результаты кластеризации зависят от способа измерения объектов. Если одни объекты имеют больший разброс значений, чем другие, то при вычислении расстояний будут преобладать элементы с более широкими диапазонами.

Распространенное преобразование называется стандартизацией по Z-оценке: из значения признака Х вычитается среднее арифметическое, а результат делится на стандартное отклонение Х.

$$ X_{new} = \frac{X - Mean(X)}{StDev(X)}$$
Сохраним имена переводчиков в отдельный вектор, а затем трансформируе данные.

```{r}
expected <- corpus_mx$translator
expected
```

```{r}
corpus_mx <- corpus_mx |> 
  select(-translator) |> 
  scale()
  
round(colMeans(corpus_mx), 1)
```

Можно приступать к кластеризации. 

### Кластеризация по методу K средних 

Алгоритм кластеризации:

1. Каждому наблюдению присваивается случайно выбранное число из интервала от 1 до K (число кластеров). Это исходные метки

![](https://delladata.fr/wp-content/uploads/2020/05/algo_kmeans.jpg.webp)

2. Вычисляется _центроид_ для каждого из кластеров. Центроид k-го класса -- вектор из p средних значений признаков, описывающих наблюдения из этого кластера;

3. Каждому наблюдению присваивается метка того кластера, чей центроид находится ближе всего к этому наблюдению (удаленность выражается обычно в виде евклидова расстояния)

4. Повторить шаги 2-3 до тех пор, пока метки классов не станут изменяться. 

Это дает возможность минимизировать **внутрикластерный разброс**: хорошей считается такая кластеризация, при которой такой разброс минимален. 

Когда центроиды двигаются, кластеры приобретают и теряют документы.


![](https://www.tidymodels.org/learn/statistics/k-means/kmeans.gif)

Внутрикластерный разброс в кластере k -- это сумма квадратов евклидовых расстояний между всеми парами наблюдений в этом кластере, разделенная на общее число входящих в него наблюдений.

```{r}
set.seed(02062024)
# установим число центров по числу переводчиков
km_out <- kmeans(corpus_mx, centers = 6, nstart = 20)

tibble(expected = expected, 
       predicted = km_out$cluster) %>% 
  group_by(expected) %>% 
  count(predicted)
```

Как это понимать?

### Визуализация кластеров

Кластеризацию по методу k-средних можно совместить со снижением размерности; это позволит показать наши кластеры на диаграмме. 

```{r}
library(broom)

pca_fit <- prcomp(corpus_mx, scale = F, center = F)

pca_fit %>%
  augment(corpus_mx) %>% 
  ggplot(aes(.fittedPC1, .fittedPC2, 
             color = expected, shape = as.factor(km_out$cluster))) +
  geom_point(size = 3, alpha = 0.7)
```

Что все это значит?

### Иерархическая кластеризация



## Векторизация

```{r}
set.seed(20240602)
library(word2vec)
model <- word2vec(x = corpus_w$word)
embedding <- as.matrix(model)
dim(embedding)
predict(model, c("дружба", "казнь", "учитель"), type = "nearest", top_n = 10) |> 
  bind_rows()
```

Получившуюся модель можно визуализировать. Для этого многомерное пространство нужно уменьшить до двух. Мы будем использовать алгоритм UMAP:

```{r umap}
#| cache: true

library(uwot)
viz <- umap(embedding, n_neighbors = 15, n_threads = 2)
dim(viz)
```


```{r}
#| fig-width: 15
#| fig-height: 15

tibble(word = rownames(embedding), 
       V1 = viz[, 1], 
       V2 = viz[, 2]) |> 
  ggplot(aes(x = V1, y = V2, label = word)) + 
  geom_text(size = 2)
```

## Ссылки {.unnumbered}